#This patch was developed both in-house and from outside. We plan to submit it
#upstream, but do not yet have a target date for doing so
#
# HG changeset patch
# Parent  d0515cffc4115654bc7bc509ce1947df440b79de
29844378 rds-stress enhancements for testing process hangs during munmap

diff -r d0515cffc411 rds-stress.c
--- a/rds-stress.c	Tue Jun 11 09:37:52 2019 -0700
+++ b/rds-stress.c	Tue Jun 11 09:46:55 2019 -0700
@@ -1008,15 +1008,95 @@
 	uint8_t			rdma_next_op;
 };
 
+/*
+ * Here is the usage for the new test option "--rdma-test-munmap N""
+ *
+ *   Receiver:
+ *	rds-stress -r <local ip>
+ *
+ *   Sender:
+ *	rds-stress -s <remote ip> -r <local ip> -D <rdma len> -t <tasks> --rdma-test-munmap N
+ *
+ * data for --rdma-test-munmap N option
+ *   1 - allocate 4M aligned buffer, munmap start of RDMA buffer
+ *   2 - allocate 4M aligned buffer, munmap middle of RDMA buffer
+ *   3 - allocate 4M aligned buffer, munmap end of RDMA buffer
+ *   4 - munmap start of RDMA buffer
+ *   5 - munmap middle of RDMA buffer
+ *   6 - munmap end of RDMA buffer
+ *
+ * <rdma len> must be a multiple of PAGESIZE (8K on SPARC, 4K on Intel).
+ *
+ * This test option is being added as it is by far the easiest way to
+ * exercise the Solaris code that is being fixed for Bug 29552286.
+ * That bug is triggered by munmap() of memory that is locked down
+ * for rdsv3 rdma operation.
+ *
+ * Beware, output is very chaotic.  Here is the simplest case, 1 task:
+ *
+ *     root@etc4mzadm0201:~# rds-stress -r 192.168.10.7
+ *
+ *     root@etc4mzadm0301:~# rds-stress -s 192.168.10.7 -r 192.168.10.5 -D 480k -t 1 --rdma-test-munmap 1
+ *     connecting to 192.168.10.7:4000
+ *     negotiated options, tasks will start in 2 seconds
+ *     len 0xf2000 rounded up to 0x400000
+ *     RDMA BASE = 0xffffffff7e400000
+ *     Only the FIRST part of the buffer will be munmap'ed
+ *     getsockopt(SNDBUF) returned 2174, we wanted 2174 * 2
+ *     getsockopt(RCVBUF) returned 2174, we need 2174 * 2
+ *     Starting up..munmap(ffffffff7e400000)
+ *     Successful munmap
+ *     Exiting.....
+ *     tsks   tx/s   rx/s  tx+rx K/s    mbi K/s    mbo K/s tx us/c   rtt us cpu %
+ *        1      0      0       0.00       0.00       0.00    0.00     0.00 -1.00
+ *     ---------------------------------------------
+ *        1      0      0       0.00       0.00       0.00    0.00     0.00 -1.00  (average)
+ *     root@etc4mzadm0301:~#
+ *
+ * Note that sometimes you have to interrupt rds-stress.11.3 to get it
+ * to stop.  This is nothing new, just part of knowing rds-stress.
+ *
+ * What you should see is one or more lines of output from:
+ *
+ *     root@etc4mzadm0301:~# echo *rdsv3_debug_buf/s|mdb -k|grep Unusual
+ *     rdsv3_ib_umem_cb:       Unusual: mr: c4071084f978, umem_cookie: c406013562d0
+ *     root@etc4mzadm0301:~#
+ *
+ * which indicates the fix is working.  Note that rdsv3_debug_buf is all too
+ * frequently written to on 11.3, so the buffer may wrap around, erasing the
+ * "Unusual" line before you run mdb.
+ *
+ * Another indication is to check for hung threads with:
+ *
+ *     root@etc4mzadm0301:~# echo "::stacks -c as_unmap_impl" | mdb -k
+ *     root@etc4mzadm0301:~#
+ *
+ * If any stack traces appear, you should rerun that command to make sure that
+ * they were intermittent because if they persist, we have a problem.
+ */
+int rdma_munmap_option = 0;
+int rdma_alloc_4M = 0;
+int rdma_test_munmap_start = 0;
+int rdma_test_munmap_middle = 0;
+int rdma_test_munmap_end = 0;
+caddr_t rdma_test_start_address;
+caddr_t rdma_test_end_address;
+
 static void alloc_rdma_buffers(struct task *t, struct options *opts)
 {
 	unsigned int i, j;
 	size_t len;
 	caddr_t	base;
+	int reverse = rdma_test_munmap_end;
 
 	/* We use mmap here rather than malloc, because it is always
 	 * page aligned. */
 	len = 2 * opts->nr_tasks * opts->req_depth * (opts->rdma_vector * opts->rdma_size) + sys_page_size;
+	if (rdma_alloc_4M) {
+		size_t old_len = len;
+		len = (len + 0x400000 - 1) & ~(0x400000 - 1);
+		printf("len 0x%x rounded up to 0x%x\n", old_len, len);
+	}
 #if defined(__SVR4) && defined(__sun)
 	base = mmap(NULL, len, PROT_READ|PROT_WRITE, MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
 #else	
@@ -1026,14 +1106,38 @@
 		die_errno("alloc_rdma_buffers: mmap failed");
 	memset(base, 0x2f, len);
 	base += opts->rdma_alignment;
+	if (rdma_munmap_option)
+		printf("RDMA BASE = 0x%p\n", base);
+	if (reverse) {
+		printf("buffers will be allocated from the end, not the start\n");
+		base += len;
+		printf("RDMA END = 0x%p\n", base);
+	}
 
 	for (i = 0; i < opts->nr_tasks; ++i, ++t) {
 		for (j = 0; j < opts->req_depth; ++j) {
+			if (reverse)
+				base -= opts->rdma_size * opts->rdma_vector;
 			t->rdma_buf[j] = (uint64_t *) base;
-			base += opts->rdma_size * opts->rdma_vector;
-
+			if (i == 0 && j == 0) {
+				if (rdma_test_munmap_start) {
+					printf("Only the FIRST part of the buffer will be munmap'ed\n");
+					rdma_test_start_address = base;
+				} else if (rdma_test_munmap_middle) {
+					printf("Only the MIDDLE part of the buffer will be munmap'ed\n");
+				} else if (rdma_test_munmap_end) {
+					printf("Only the LAST part of the buffer will be munmap'ed\n");
+					rdma_test_end_address = base;
+				}
+			}
+			if (!reverse)
+				base += opts->rdma_size * opts->rdma_vector;
+
+			if (reverse)
+				base -= opts->rdma_size * opts->rdma_vector;
 			t->local_buf[j] = (uint64_t *) base;
-			base += opts->rdma_size * opts->rdma_vector;
+			if (!reverse)
+				base += opts->rdma_size * opts->rdma_vector;
 
 			t->rdma_req_key[j] = 0;
 			t->rdma_inflight[j] = 0;
@@ -1334,6 +1438,19 @@
 	rdma_put_cmsg(msg, RDS_CMSG_RDMA_MAP, &args, sizeof(args));
 }
 
+int
+check_rdma_munmap(caddr_t addr)
+{
+	if (rdma_test_munmap_start && addr == rdma_test_start_address)
+		return (1);
+	if (rdma_test_munmap_end && addr == rdma_test_end_address)
+		return (1);
+	if (rdma_test_munmap_middle && addr != rdma_test_start_address &&
+	    addr != rdma_test_end_address)
+		return (1);
+	return (0);
+}
+
 static void rdma_process_ack(int fd, struct header *hdr,
 		struct child_control *ctl)
 {
@@ -1344,8 +1461,17 @@
 		  (unsigned long long) hdr->rdma_addr);
 
 	/* Need to free the MR unless allocated with use_once */
-	if (!opt.rdma_use_once && !opt.rdma_cache_mrs)
+	if (!opt.rdma_use_once && !opt.rdma_cache_mrs) {
+		if (rdma_munmap_option && check_rdma_munmap(hdr->rdma_addr)) {
+			fprintf(stderr, "munmap(%p)\n", hdr->rdma_addr);
+			if (munmap(hdr->rdma_addr, hdr->rdma_size) == 0) {
+				fprintf(stderr, "Successful munmap\nExiting...\n");
+				exit(0);
+			}
+		}
+
 		free_rdma_key(fd, hdr->rdma_key);
+	}
 
 	/* if acking an rdma write request - then remote node wrote local host buffer
 	 * (data in) so count this as rdma data coming in (rdma_read) - else remote node read
@@ -3053,6 +3179,37 @@
 		die("%s must be at least %u bytes\n", desc, max);
 }
 
+void setup_rdma_munmap_test()
+{
+	if (rdma_munmap_option == 0)
+		rdma_munmap_option = 1;
+
+	switch (rdma_munmap_option) {
+	default:
+	case 1:
+		rdma_alloc_4M = 1;
+		rdma_test_munmap_start = 1;
+		break;
+	case 2:
+		rdma_alloc_4M = 1;
+		rdma_test_munmap_middle = 1;
+		break;
+	case 3:
+		rdma_alloc_4M = 1;
+		rdma_test_munmap_end = 1;
+		break;
+	case 4:
+		rdma_test_munmap_start = 1;
+		break;
+	case 5:
+		rdma_test_munmap_middle = 1;
+		break;
+	case 6:
+		rdma_test_munmap_end = 1;
+		break;
+	}
+}
+
 enum {
 	OPT_RDMA_USE_ONCE = 0x100,
 	OPT_RDMA_USE_GET_MR,
@@ -3069,6 +3226,7 @@
         OPT_SHOW_HISTOGRAM,
 	OPT_RESET,
 	OPT_ASYNC,
+	OPT_RDMA_TEST_MUNMAP,
 };
 
 static struct option long_options[] = {
@@ -3096,6 +3254,7 @@
 { "rdma-cache-mrs",	required_argument,	NULL,	OPT_RDMA_CACHE_MRS },
 { "rdma-alignment",	required_argument,	NULL,	OPT_RDMA_ALIGNMENT },
 { "rdma-key-o-meter",	no_argument,		NULL,	OPT_RDMA_KEY_O_METER },
+{ "rdma-test-munmap",	required_argument,	NULL,	OPT_RDMA_TEST_MUNMAP },
 { "show-params",	no_argument,		NULL,	OPT_SHOW_PARAMS },
 { "show-perfdata",	no_argument,		NULL,	OPT_PERFDATA },
 { "connect-retries",	required_argument,	NULL,	OPT_CONNECT_RETRIES },
@@ -3265,6 +3424,10 @@
 			case OPT_RDMA_KEY_O_METER:
 				opts.rdma_key_o_meter = 1;
 				break;
+			case OPT_RDMA_TEST_MUNMAP:
+				rdma_munmap_option = parse_ull(optarg, 6);
+				setup_rdma_munmap_test();
+				break;
 			case OPT_SHOW_PARAMS:
 				opts.show_params = 1;
 				break;
